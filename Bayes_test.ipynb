{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import tensorflow as tf\n",
    "from my_util import show_graph, reset_graph\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.9  \n",
    "import copy\n",
    "import time\n",
    "\n",
    "def random_dist(n):\n",
    "    '''A method for randomly sampling a distribution over n+1 states. Only the n probabilities are returned.'''\n",
    "    a = sorted([0.0] + list(np.random.random_sample(n)))\n",
    "    a = np.array([a[i + 1] - a[i] for i in range(n)], dtype = np.float64)    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a vertex class that deals with vertex specific data and methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vertex():\n",
    "    def __init__(self, name):\n",
    "        self.vname = name\n",
    "        self.parents = []\n",
    "        self.hidden = False\n",
    "        self.states = 0\n",
    "        self.state = None\n",
    "        self.dist = {}\n",
    "        self._set_dist = {}\n",
    "        self.trans = {}\n",
    "        self._set_trans = {}       \n",
    "        \n",
    "    def add_parent(self, parent):\n",
    "        '''Add a parent in the DAG'''\n",
    "        self.parents.append(parent)\n",
    "        \n",
    "    def init_dist(self, train_thres):\n",
    "        '''Initialize distribution for a vertex without parents with uniform distribution over the states. Number of states minus 1 tf variables are defined for the probabilities, where the last probability is determined by the rest. A tf operation for modifying the distribution is constructed.\n",
    "        '''\n",
    "        atom = np.float64(sp.sympify('1/'+str(self.states)))\n",
    "        temp = []\n",
    "        for state in range(self.states - 1):\n",
    "            self.dist[state] = tf.Variable(atom, name = self.vname, dtype = tf.float64, trainable = self.hidden or (np.random.random()<train_thres))\n",
    "            temp.append(self.dist[state])\n",
    "            self._set_dist[state] = tf.assign(self.dist[state], X, validate_shape = True)\n",
    "\n",
    "        self.dist[self.states - 1] = 1 - tf.add_n(temp)\n",
    "    \n",
    "    def pa_states(self):\n",
    "        '''Return the parents' states'''\n",
    "        temp = []\n",
    "        for v in self.parents:\n",
    "            temp.append(v.state)\n",
    "        return temp\n",
    "    \n",
    "    def enum_pa_states(self, lev):\n",
    "        '''Define a recursive generator for all combinations of parents' states'''\n",
    "        if lev == len(self.parents):\n",
    "            yield []\n",
    "        else:    \n",
    "            for i in range(self.parents[lev].states):\n",
    "                for j in self.enum_pa_states(lev + 1):\n",
    "                    yield [i] + j\n",
    "                \n",
    "    \n",
    "    def init_trans(self, train_thres):\n",
    "        '''Initialize conditional probabilities for a vertex with parents with uniform distribution over the states. Per combination of parents' states, number of states minus 1 tf variables are defined for the probabilities, where the last probability is determined by the rest, and a tf operation for modifying the conditional probabilities is constructed.\n",
    "        '''\n",
    "        atom = np.array(sp.sympify('1/'+str(self.states)), dtype = np.float64)\n",
    "        for comb in self.enum_pa_states(0):\n",
    "            temp = []\n",
    "            for state in range(self.states - 1):\n",
    "                tup = tuple(comb + [state])\n",
    "                self.trans[tup] = tf.Variable(atom, name = self.vname, dtype = tf.float64, trainable = self.hidden or (np.random.random()<train_thres))\n",
    "                temp.append(self.trans[tup])\n",
    "                self._set_trans[tup] = tf.assign(self.trans[tup], X, validate_shape = True)\n",
    "            tup = tuple(comb + [self.states - 1])\n",
    "            self.trans[tup] = 1 - tf.add_n(temp)\n",
    "        \n",
    "    def set_dist(self, dist):\n",
    "        '''Run the operation of modifying distributions in a tf session'''\n",
    "        for state in range(self.states - 1):\n",
    "            self._set_dist[state].eval(feed_dict = {X : dist[state]})\n",
    "    \n",
    "    def set_trans(self, comb, prob):\n",
    "        '''Run the operation of modifying conditional probabilities in a tf session'''\n",
    "        for state in range(self.states - 1):\n",
    "            self._set_trans[tuple(comb + [state])].eval(feed_dict = {X : prob[state]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the main class for the Bayesian Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianNetwork():\n",
    "    def __init__(self, filename = None, train_thres = 1.0):\n",
    "        '''The input file has the format:\n",
    "        Line 1: Vertex names separated by commas;\n",
    "        Line 2: Indicator of hidden ('h') or observable ('o') for each vertex in order;\n",
    "        Line 3-: One line for each vertex starting with name and comma,then either 'Root' if it has no parents or comma-separated list of parents.\n",
    "        \n",
    "        The train_thres is a parameter for experiment. Variables associated to non-root vertices have a probabilty train_thres to be trainable. By default, all parameters are trainable.\n",
    "        '''\n",
    "        self.vlist = OrderedDict()\n",
    "        self.marg = {}\n",
    "        if filename != None:\n",
    "            f = open(filename, 'r')\n",
    "            line = f.readline()\n",
    "            items = line.split(',')\n",
    "            for i in items:\n",
    "                v = i.strip()\n",
    "                self.vlist[v] = Vertex(v)\n",
    "                \n",
    "            line = f.readline()\n",
    "            items = line.split(',')\n",
    "            for (i, v), j in zip(self.vlist.items(), items):\n",
    "                s = int(j)\n",
    "                v.states = s\n",
    "                \n",
    "            line = f.readline()\n",
    "            items = line.split(',')\n",
    "            for (i, v), j in zip(self.vlist.items(), items):\n",
    "                s = (j == 'h')\n",
    "                v.hidden = s   \n",
    "                \n",
    "            line = f.readline()            \n",
    "            while line != '':\n",
    "                items = line.split(',')\n",
    "                ch = self.vlist[items[0].strip()]\n",
    "                pa = items[1].strip()\n",
    "                trainable = ch.hidden\n",
    "                if pa != 'Root': \n",
    "                    for i in range(1, len(items)):\n",
    "                        ch.add_parent(self.vlist[items[i].strip()])\n",
    "                    ch.init_trans(train_thres = train_thres)    \n",
    "                else:\n",
    "                    ch.init_dist(train_thres = train_thres)\n",
    "                line = f.readline()\n",
    "            f.close()\n",
    "        self._vlist = list(self.vlist)\n",
    "    \n",
    "    def set_hidden(self, hidden):\n",
    "        '''A helper method for setting exactly the vertex in 'hidden' as hidden vertices.\n",
    "        '''\n",
    "        for i, v in self.vlist.items():\n",
    "            v.hidden = False\n",
    "        for i in hidden:\n",
    "            self.vlist[i].hidden = True\n",
    "\n",
    "    def read_prob(self, filename):\n",
    "        '''Read the vertex-wise distributions and conditional probabilities from a file.\n",
    "        Each line starts with the vertex name, a combination of all parents' states if exist any, then a list of number of states - 1 numbers indicating the probabilities. See the accompanying input files for illustrations.\n",
    "        The probabilities can be expressed as any mathematical expressions.\n",
    "        \n",
    "        e.g. For Allman et al. 2015 Table 1 Parameter set (2)\n",
    "        \n",
    "        a, 1/5\n",
    "        b, 0, 4/5\n",
    "        b, 1, 7/10\n",
    "        c, 0,0,2/5\n",
    "        c, 0,1,9/10\n",
    "        c, 1,0,4/5\n",
    "        c, 1,1,3/5\n",
    "        ...\n",
    "        '''\n",
    "        if filename != None:\n",
    "            f = open(filename, 'r')                \n",
    "            line = f.readline()            \n",
    "            while line != '':\n",
    "                items = line.split(',')\n",
    "                ch = self.vlist[items[0].strip()]\n",
    "                if len(ch.parents) == 0: \n",
    "                    ch.set_dist(sp.matrix2numpy(sp.Matrix(sp.sympify((items[1:]))), np.float64).T[0]) \n",
    "                else:\n",
    "                    ch.set_trans(list(map(int, items[1:len(ch.parents)+1])), sp.matrix2numpy(sp.Matrix(sp.sympify(items[len(ch.parents)+1:])), np.float64).T[0])\n",
    "                line = f.readline()\n",
    "            f.close()\n",
    "    \n",
    "    def parse(self, filename):\n",
    "        '''\n",
    "        A helper method for parsing the probabilities file, used only in read_prob_interpolate below.\n",
    "        '''\n",
    "        content = []\n",
    "        f = open(filename, 'r')                \n",
    "        line = f.readline()            \n",
    "        while line != '':\n",
    "            items = line.split(',')\n",
    "            ch = self.vlist[items[0].strip()]\n",
    "            if len(ch.parents) == 0: \n",
    "                content.append((ch, sp.matrix2numpy(sp.Matrix(sp.sympify((items[1:]))), np.float64).T[0]))\n",
    "            else:\n",
    "                content.append((ch, list(map(int, items[1:len(ch.parents)+1])), sp.matrix2numpy(sp.Matrix(sp.sympify(items[len(ch.parents)+1:])), np.float64).T[0]))\n",
    "            line = f.readline()\n",
    "        f.close()\n",
    "        return content\n",
    "    \n",
    "    def read_prob_interpolate(self, filename1, filename2, size = 100):\n",
    "        '''\n",
    "        Interpolate between two sets of parameters linearly.\n",
    "        '''\n",
    "        file1 = self.parse(filename1)\n",
    "        file2 = self.parse(filename2)\n",
    "        for i in range(size + 1):\n",
    "            for v1, v2 in zip(file1, file2):\n",
    "                if len(v1[0].parents) == 0:\n",
    "                    v1[0].set_dist((v1[1]*i+v2[1]*(size - i))/size)\n",
    "                else:\n",
    "                    v1[0].set_trans(v1[1], (v1[2]*i+v2[2]*(size - i))/size)\n",
    "            print(i, target_loss.eval())\n",
    "        return\n",
    "    \n",
    "    def sample_prob(self):\n",
    "        '''\n",
    "        Randomly sample an entire set of underlying parameters.\n",
    "        '''\n",
    "        for _, v in self.vlist.items():\n",
    "            if len(v.parents) == 0:\n",
    "                v.set_dist(random_dist(v.states - 1))\n",
    "            else:\n",
    "                for comb in v.enum_pa_states(0):\n",
    "                    v.set_trans(comb, random_dist(v.states - 1))\n",
    "        return\n",
    "    \n",
    "    def output_prob(self, filename, reverse = False):\n",
    "        '''\n",
    "        Output the current probabilities in a tf session to a file in the same format as input files.\n",
    "        If reverse is True, flip the hidden labels assuming they are binary.\n",
    "        '''\n",
    "        f = open(filename, 'w')\n",
    "        for _, v in self.vlist.items():\n",
    "            if len(v.parents) == 0:\n",
    "                f.write(v.vname)\n",
    "                for state in range(v.states - 1):\n",
    "                    if v.hidden and reverse:\n",
    "                        f.write(\",\"+str(1 - v.dist[state].eval()))\n",
    "                    else:\n",
    "                        f.write(\",\"+str(v.dist[state].eval()))\n",
    "                f.write('\\n')\n",
    "            else:\n",
    "                for comb in v.enum_pa_states(0):\n",
    "                    f.write(v.vname)\n",
    "                    comb2 = comb\n",
    "                    if v.parents[0].hidden and reverse:\n",
    "                        comb2 = copy.deepcopy(comb)\n",
    "                        comb[0] = 1-comb[0]\n",
    "                    for j in comb2:\n",
    "                        f.write(\",\"+str(j))\n",
    "                    for state in range(v.states - 1):\n",
    "                        f.write(','+str(v.trans[tuple(comb + [state])].eval()))\n",
    "                    f.write('\\n')\n",
    "        f.close()    \n",
    "        return\n",
    "            \n",
    "    def get_constraint_loss(self):\n",
    "        '''\n",
    "        Construct a loss function to constrain all underlying parameters to be legal probabilities.\n",
    "        '''\n",
    "        temp = []\n",
    "        for _, v in self.vlist.items():\n",
    "            if len(v.parents) == 0:\n",
    "                for _, i in v.dist.items():\n",
    "                    temp.append(-(tf.minimum(i, 0)))\n",
    "                    temp.append((tf.maximum(i-1, 0)))\n",
    "            else:\n",
    "                for _, i in v.trans.items():\n",
    "                    temp.append(-(tf.minimum(i, 0)))\n",
    "                    temp.append((tf.maximum(i-1, 0)))\n",
    "        return tf.add_n(temp)\n",
    "    \n",
    "    def get_target_loss(self, target):\n",
    "        '''\n",
    "        Construct a loss function for the distance between the current marginal joint distribution on the observable vertices and the target observable distribution\n",
    "        '''\n",
    "        temp = []\n",
    "        for obs, prob in self.marg.items():\n",
    "            temp += [tf.abs(prob - target[obs])]\n",
    "        return tf.add_n(temp)\n",
    "    \n",
    "    def compute_prob(self):\n",
    "        '''\n",
    "        Compute the total probability of a single configuation.\n",
    "        '''\n",
    "        temp = []\n",
    "        for _, v in self.vlist.items():\n",
    "            if len(v.parents) == 0:\n",
    "                temp.append(v.dist[v.state])\n",
    "            else:\n",
    "                temp.append(v.trans[tuple(v.pa_states()+[v.state])])\n",
    "        return tf.reduce_prod(temp)\n",
    "    \n",
    "    def enum_hidden(self, vi, prob_list):\n",
    "        '''\n",
    "        Given visible states, enumerate all hidden states.\n",
    "        Call the method with vi = 0.\n",
    "        prob_list is a callback list for storing the total probabilities.\n",
    "        '''\n",
    "        if vi == len(self.vlist):    \n",
    "            prob_list.append(self.compute_prob())\n",
    "            return\n",
    "        else:   \n",
    "            v = self.vlist[self._vlist[vi]]\n",
    "            if v.hidden:\n",
    "                for i in range(v.states):\n",
    "                    v.state = i\n",
    "                    self.enum_hidden(vi + 1, prob_list)\n",
    "            else:\n",
    "                self.enum_hidden(vi + 1, prob_list)\n",
    "            return\n",
    "                    \n",
    "    \n",
    "    def compute_observable(self, vi):\n",
    "        '''\n",
    "        Construct the tf operation for computing the marginal joint distributions over visible vertices.\n",
    "        Call the method with vi = 0.\n",
    "        '''\n",
    "        if vi == len(self.vlist):\n",
    "            obs = []\n",
    "            for _, v in self.vlist.items():\n",
    "                if not v.hidden:\n",
    "                    obs.append(v.state)\n",
    "            obs = tuple(obs)\n",
    "            temp = []\n",
    "            self.enum_hidden(0, temp)\n",
    "            self.marg[obs] = tf.add_n(temp)    \n",
    "            return\n",
    "        else:\n",
    "            v = self.vlist[self._vlist[vi]]\n",
    "            if not v.hidden:\n",
    "                for i in range(v.states):\n",
    "                    v.state = i\n",
    "                    self.compute_observable(vi + 1)\n",
    "            else:\n",
    "                self.compute_observable(vi + 1)\n",
    "            return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us check the result using Allman et al. 2015 Table 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph(seed=int(time.time()))\n",
    "X = tf.placeholder(tf.float64)\n",
    "G = BayesianNetwork('graph 4-3e.txt', 1.0)\n",
    "init = tf.global_variables_initializer()\n",
    "G.compute_observable(0)\n",
    "cons_loss = G.get_constraint_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using parameter set (1), we can compute the observation distributions. Here the states 0 and 1 correspond to states 1 and 2 in the paper. The tuple correponds to the states of vertices 1,2,3,4 in the paper in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0) 0.1856\n",
      "(0, 0, 0, 1) 0.05119999999999999\n",
      "(0, 0, 1, 0) 0.2048\n",
      "(0, 0, 1, 1) 0.07039999999999999\n",
      "(0, 1, 0, 0) 0.05439999999999999\n",
      "(0, 1, 0, 1) 0.020799999999999992\n",
      "(0, 1, 1, 0) 0.0832\n",
      "(0, 1, 1, 1) 0.0496\n",
      "(1, 0, 0, 0) 0.05399999999999999\n",
      "(1, 0, 0, 1) 0.0252\n",
      "(1, 0, 1, 0) 0.0684\n",
      "(1, 0, 1, 1) 0.032400000000000005\n",
      "(1, 1, 0, 0) 0.0312\n",
      "(1, 1, 0, 1) 0.013600000000000003\n",
      "(1, 1, 1, 0) 0.0384\n",
      "(1, 1, 1, 1) 0.016800000000000002\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    G.read_prob('prob 4-3e a.txt')\n",
    "    target_tb = sess.run(G.marg)\n",
    "    for i, j in target_tb.items():\n",
    "        print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look accurate except the last digit. We can rewrite them as fractions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0) 116/625\n",
      "(0, 0, 0, 1) 32/625\n",
      "(0, 0, 1, 0) 128/625\n",
      "(0, 0, 1, 1) 44/625\n",
      "(0, 1, 0, 0) 34/625\n",
      "(0, 1, 0, 1) 13/625\n",
      "(0, 1, 1, 0) 52/625\n",
      "(0, 1, 1, 1) 31/625\n",
      "(1, 0, 0, 0) 27/500\n",
      "(1, 0, 0, 1) 63/2500\n",
      "(1, 0, 1, 0) 171/2500\n",
      "(1, 0, 1, 1) 81/2500\n",
      "(1, 1, 0, 0) 39/1250\n",
      "(1, 1, 0, 1) 17/1250\n",
      "(1, 1, 1, 0) 24/625\n",
      "(1, 1, 1, 1) 21/1250\n"
     ]
    }
   ],
   "source": [
    "for i, j in target_tb.items():\n",
    "    print(i, sp.Integer(sp.Expr.round(sp.sympify(j)*1000000,0))/1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exactly matches the tables at the bottom of Table 1 in the paper. Due to the way they are written in the paper, the order of reading their tables is the first element of each table, and then the second element of each table, etc.\n",
    "\n",
    "We can check the observable distribution using parameter set (2), and verify that it is exactly the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 0, 0) 116/625\n",
      "(0, 0, 0, 1) 32/625\n",
      "(0, 0, 1, 0) 128/625\n",
      "(0, 0, 1, 1) 44/625\n",
      "(0, 1, 0, 0) 34/625\n",
      "(0, 1, 0, 1) 13/625\n",
      "(0, 1, 1, 0) 52/625\n",
      "(0, 1, 1, 1) 31/625\n",
      "(1, 0, 0, 0) 27/500\n",
      "(1, 0, 0, 1) 63/2500\n",
      "(1, 0, 1, 0) 171/2500\n",
      "(1, 0, 1, 1) 81/2500\n",
      "(1, 1, 0, 0) 39/1250\n",
      "(1, 1, 0, 1) 17/1250\n",
      "(1, 1, 1, 0) 24/625\n",
      "(1, 1, 1, 1) 21/1250\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    G.read_prob('prob 4-3e b.txt')\n",
    "    target_tb = sess.run(G.marg)\n",
    "    for i, j in target_tb.items():\n",
    "        print(i, sp.Integer(sp.Expr.round(sp.sympify(j)*1000000,0))/1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check numerical stability, we run the gradient descent from a global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_loss = G.get_target_loss(target_tb)\n",
    "loss = target_loss + cons_loss\n",
    "learning_rate = tf.placeholder(tf.float64)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "adamoptimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "adam_training_op = adamoptimizer.minimize(loss)\n",
    "init2 = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that Adadelta optimizer with a learning rate depending on the loss works better than other optimizers with adaptive learning rates, which often do not even converge starting from a global minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 5.898059818321144e-17\n",
      "9.823684224939677e-07\n",
      "1.4323991980383366e-08\n",
      "5.363384510820302e-10\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init2.run()\n",
    "    G.read_prob('prob 4-3e a.txt')\n",
    "    lr = 0.001\n",
    "    counter = 0\n",
    "    print('initial loss:', target_loss.eval())\n",
    "    while True:\n",
    "        for i in range(200):\n",
    "            training_op.run(feed_dict = {learning_rate: lr})\n",
    "        temp = target_loss.eval()  \n",
    "        print(temp)\n",
    "        lr = min(temp*10, lr)\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            saver.save(sess, \"Bayes/Save.ckpt\")    \n",
    "        if (temp < 1e-9) or (counter >= 10):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to AdamOptimizer starting from the global minimum. Here the loss diverged so that the dynamic learning rate actually never kicked in. Changing initial and dynamic learning rates do not help either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 5.898059818321144e-17\n",
      "0.0005646576053426871\n",
      "0.00048705921434175244\n",
      "0.0006796705922605395\n",
      "0.0002459418997772158\n",
      "0.0005607448813293309\n",
      "0.0005257245066200428\n",
      "0.0005884374652615158\n",
      "0.0003761820181770563\n",
      "0.0003863891414351648\n",
      "0.0004643808020423131\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init2.run()\n",
    "    G.read_prob('prob 4-3e a.txt')\n",
    "    lr = 0.001\n",
    "    counter = 0\n",
    "    print('initial loss:', target_loss.eval())\n",
    "    while True:\n",
    "        for i in range(200):\n",
    "            adam_training_op.run(feed_dict = {learning_rate: lr})\n",
    "        temp = target_loss.eval()  \n",
    "        print(temp)\n",
    "        lr = min(temp*10, lr)\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            saver.save(sess, \"Bayes/Save.ckpt\")    \n",
    "        if (temp < 1e-9) or (counter >= 10):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we modify the starting parameter slightly, by adding 0.1 to the single parameter on the hidden vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 0.12746666666666656\n",
      "0.08576381252509549\n",
      "0.03818969678199914\n",
      "0.009597639408994727\n",
      "0.006143380483190148\n",
      "0.005446575377535087\n",
      "0.005182456839372067\n",
      "0.004870466710865081\n",
      "0.004567360510793834\n",
      "0.004371106241790497\n",
      "0.004266719351624152\n",
      "0.00417619932454827\n",
      "0.004069039243926098\n",
      "0.0039833619677277065\n",
      "0.003876263341692743\n",
      "0.0037817280431184912\n",
      "0.0036758011220463013\n",
      "0.003582163271944702\n",
      "0.0034831825689687207\n",
      "0.003411971838788148\n",
      "0.003372190840244864\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init2.run()\n",
    "    G.read_prob('prob 4-3e a perturbed.txt')\n",
    "    lr = 0.1\n",
    "    counter = 0\n",
    "    print('initial loss:', target_loss.eval())\n",
    "    while True:\n",
    "        for i in range(200):\n",
    "            training_op.run(feed_dict = {learning_rate: lr})\n",
    "        temp = target_loss.eval()  \n",
    "        print(temp)\n",
    "        lr = min(temp*10, lr)\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            saver.save(sess, \"Bayes/Save.ckpt\")    \n",
    "        if (temp < 1e-9) or (counter >= 20):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we changed the initial learning rate to 0.1 to speed up training, but dynamic learning rate kicked in later.\n",
    "\n",
    "By running long enough, we see that the parameters no longer converge to the global minimum. I tried tuning the initial learning rate as well as the dynamic learning rate, but the same failure happens.\n",
    "\n",
    "To see closer what is happening, let us make the hidden vertex the only trainable variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'a:0' shape=() dtype=float64_ref>]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph(seed=int(time.time()))\n",
    "X = tf.placeholder(tf.float64)\n",
    "G = BayesianNetwork('graph 4-3e.txt', 0.0)\n",
    "G.compute_observable(0)\n",
    "cons_loss = G.get_constraint_loss()\n",
    "target_loss = G.get_target_loss(target_tb)\n",
    "loss = target_loss + cons_loss\n",
    "learning_rate = tf.placeholder(tf.float64)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11415402224860025\n",
      "0.09855516169729601\n",
      "0.0809940668983128\n",
      "0.0616696558894132\n",
      "0.04073024667618916\n",
      "0.01829198034509806\n",
      "5.3746096042354974e-05\n",
      "4.80493770178142e-07\n",
      "4.994527744509036e-09\n",
      "4.403401428210163e-11\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    G.read_prob('prob 4-3e a perturbed.txt')\n",
    "    lr = 0.1\n",
    "    counter = 0\n",
    "    while True:\n",
    "        for i in range(200):\n",
    "            training_op.run(feed_dict = {learning_rate: lr})\n",
    "        temp = target_loss.eval()  \n",
    "        print(temp)\n",
    "        lr = min(temp*10, lr)\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            saver.save(sess, \"Bayes/Save.ckpt\")    \n",
    "        if (temp < 1e-9) or (counter >= 20):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the single parameter did converge. If we allow a few more trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'a:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'c_3:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'd:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'e:0' shape=() dtype=float64_ref>]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph(seed=int(time.time()))\n",
    "X = tf.placeholder(tf.float64)\n",
    "G = BayesianNetwork('graph 4-3e.txt', 0.15)\n",
    "G.compute_observable(0)\n",
    "cons_loss = G.get_constraint_loss()\n",
    "target_loss = G.get_target_loss(target_tb)\n",
    "loss = target_loss + cons_loss\n",
    "learning_rate = tf.placeholder(tf.float64)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11241949110806096\n",
      "0.094840966202152\n",
      "0.07551785990049473\n",
      "0.05845540731809814\n",
      "0.04111639022832772\n",
      "0.023462957569314914\n",
      "0.004897656753087273\n",
      "7.420591771698133e-05\n",
      "1.0561646352665044e-06\n",
      "6.253732807645629e-09\n",
      "6.022641239888937e-11\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    G.read_prob('prob 4-3e a perturbed.txt')\n",
    "    lr = 0.1\n",
    "    counter = 0\n",
    "    while True:\n",
    "        for i in range(200):\n",
    "            training_op.run(feed_dict = {learning_rate: lr})\n",
    "        temp = target_loss.eval()  \n",
    "        print(temp)\n",
    "        lr = min(temp*10, lr)\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            saver.save(sess, \"Bayes/Save.ckpt\")    \n",
    "        if (temp < 1e-9) or (counter >= 20):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is still fine. Now we add more trainable variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'a:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'b:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'b_1:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'c_2:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'c_3:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'd:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'd_2:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'd_3:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'e_2:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'e_3:0' shape=() dtype=float64_ref>]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph(seed=int(time.time()))\n",
    "X = tf.placeholder(tf.float64)\n",
    "G = BayesianNetwork('graph 4-3e.txt', 0.6)\n",
    "G.compute_observable(0)\n",
    "cons_loss = G.get_constraint_loss()\n",
    "target_loss = G.get_target_loss(target_tb)\n",
    "loss = target_loss + cons_loss\n",
    "learning_rate = tf.placeholder(tf.float64)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09005131792381307\n",
      "0.04793345191120958\n",
      "0.02994913495511301\n",
      "0.023225481843268815\n",
      "0.018249773055080784\n",
      "0.014905660869650525\n",
      "0.012595204171079882\n",
      "0.010835600997813987\n",
      "0.009924203951723413\n",
      "0.008980146438960775\n",
      "0.008183128825176973\n",
      "0.007451762227071602\n",
      "0.006862895123938489\n",
      "0.0062164867035253875\n",
      "0.0056814281026376316\n",
      "0.0051865477341188175\n",
      "0.004820293368212632\n",
      "0.004422461228555824\n",
      "0.004122633816350016\n",
      "0.0038235518553833264\n",
      "0.0035106096263810074\n",
      "0.0032545547263378664\n",
      "0.0030418484122812436\n",
      "0.002919115896453049\n",
      "0.0027780285916682544\n",
      "0.002668110670405132\n",
      "0.0025614720476127197\n",
      "0.0024589141607337904\n",
      "0.00236284577186493\n",
      "0.002267588244926776\n",
      "0.002185237915973068\n",
      "0.0020941658921485463\n",
      "0.002015086103565737\n",
      "0.0019277978531020545\n",
      "0.0018533207196456455\n",
      "0.001789740633898744\n",
      "0.0017222134061891745\n",
      "0.0016553106437521784\n",
      "0.0015931224823813508\n",
      "0.001532607684098325\n",
      "0.0014798966821029918\n",
      "0.0014220868399922818\n",
      "0.0013636404721558883\n",
      "0.0013200513675523402\n",
      "0.0012623318966951593\n",
      "0.0012144065904959624\n",
      "0.0011732577612182157\n",
      "0.001129093639653432\n",
      "0.0010888605298018626\n",
      "0.001048791292611111\n",
      "0.0010107835172281105\n",
      "0.0009754869380121448\n",
      "0.0009537573158276327\n",
      "0.0009405676458483034\n",
      "0.0009326248087197751\n",
      "0.0009162554563300826\n",
      "0.0009074583570401942\n",
      "0.0008918186418380436\n",
      "0.0008783189606532005\n",
      "0.000868877976742493\n",
      "0.0008596993257235421\n",
      "0.0008487454052298338\n",
      "0.0008341964645658152\n",
      "0.0008223558702672683\n",
      "0.000812784558922303\n",
      "0.0008012954956481827\n",
      "0.000792034125404328\n",
      "0.0007828569252898484\n",
      "0.0007718224228571505\n",
      "0.0007657282089485688\n",
      "0.0007549744396576248\n",
      "0.0007451220206517815\n",
      "0.0007422390872575471\n",
      "0.0007318548873371123\n",
      "0.0007262601782151244\n",
      "0.0007173443477744151\n",
      "0.0007099883574676405\n",
      "0.0007034044706069626\n",
      "0.0006971002464533308\n",
      "0.0006908263213376131\n",
      "0.0006875133572249153\n",
      "0.0006874625441093372\n",
      "0.0006876705880855399\n",
      "0.0006838952361751371\n",
      "0.0006830364093176524\n",
      "0.0006831270790948925\n",
      "0.0006827644629433405\n",
      "0.0006797969997343143\n",
      "0.0006772761592742649\n",
      "0.0006749351775105217\n",
      "0.0006751625957999025\n",
      "0.0006727805237982977\n",
      "0.0006748396340786438\n",
      "0.0006724909049124674\n",
      "0.0006679404141175654\n",
      "0.000667683809461422\n",
      "0.0006683215876103817\n",
      "0.000666164389898501\n",
      "0.0006645965937462084\n",
      "0.000661408253510078\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    G.read_prob('prob 4-3e a perturbed.txt')\n",
    "    lr = 0.1\n",
    "    counter = 0\n",
    "    while True:\n",
    "        for i in range(200):\n",
    "            training_op.run(feed_dict = {learning_rate: lr})\n",
    "        temp = target_loss.eval()  \n",
    "        print(temp)\n",
    "        lr = min(temp*10, lr)\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            saver.save(sess, \"Bayes/Save.ckpt\")    \n",
    "        if (temp < 1e-9) or (counter >= 100):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By running long enough, this does not converge. So the high dimension caused problems by introducing either too many local minimums or saddle points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question is that whether we can detect non-identifiability with infinite preimage. When the preimage is generically infinite, we might hope that it is much easier to find a global minimum after a perturbation from the initial parameter set, which would numerically verify non-identifiability. We choose graph 4-3h, as it is non-identifiable by a large margin. (25 to 15 dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph(seed=int(time.time()))\n",
    "X = tf.placeholder(tf.float64)\n",
    "G = BayesianNetwork('graph 4-3h.txt', 1.0)\n",
    "init = tf.global_variables_initializer()\n",
    "G.compute_observable(0)\n",
    "cons_loss = G.get_constraint_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We randomly generate two sets of parameters, where the second is obtained from the first one by flipping the binary label at the hidden vertex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    G.sample_prob()\n",
    "    G.output_prob('prob 4-3h a.txt')\n",
    "    G.output_prob('prob 4-3h b.txt', reverse= True)\n",
    "    target_tb = sess.run(G.marg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_loss = G.get_target_loss(target_tb)\n",
    "loss = target_loss + cons_loss\n",
    "learning_rate = tf.placeholder(tf.float64)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "init2 = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the same perturbation the hidden vertex, by adding 0.1 to its single parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial loss: 0.1407614623564004\n",
      "0.09958150071946262\n",
      "0.05540704210828561\n",
      "0.02682988070952828\n",
      "0.021752939035413792\n",
      "0.01764618985930428\n",
      "0.015430257885244445\n",
      "0.01376670145655901\n",
      "0.012275381122024523\n",
      "0.0114396740827586\n",
      "0.01085492067780682\n",
      "0.010327203492440528\n",
      "0.00970841422455897\n",
      "0.009206414890596893\n",
      "0.008676613851552658\n",
      "0.008218180010452582\n",
      "0.00777462148451131\n",
      "0.007377771936399928\n",
      "0.006990199142466038\n",
      "0.006610868759616301\n",
      "0.006275474593536921\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init2.run()\n",
    "    G.read_prob('prob 4-3h a perturbed.txt')\n",
    "    lr = 0.1\n",
    "    counter = 0\n",
    "    print('initial loss:', target_loss.eval())\n",
    "    while True:\n",
    "        for i in range(200):\n",
    "            training_op.run(feed_dict = {learning_rate: lr})\n",
    "        temp = target_loss.eval()  \n",
    "        print(temp)\n",
    "        lr = min(temp*10, lr)\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            saver.save(sess, \"Bayes/Save.ckpt\")    \n",
    "        if (temp < 1e-9) or (counter >= 20):\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It eventually fails to find a global minimum as before. Here we could choose an even smaller perturbation, but it still does not converge to any global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now again we allow a few trainable parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'a:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'b:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'b_1:0' shape=() dtype=float64_ref>,\n",
       " <tf.Variable 'e_10:0' shape=() dtype=float64_ref>]"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_graph(seed=int(time.time()))\n",
    "X = tf.placeholder(tf.float64)\n",
    "G = BayesianNetwork('graph 4-3h.txt', 0.10)\n",
    "G.compute_observable(0)\n",
    "cons_loss = G.get_constraint_loss()\n",
    "target_loss = G.get_target_loss(target_tb)\n",
    "loss = target_loss + cons_loss\n",
    "learning_rate = tf.placeholder(tf.float64)\n",
    "optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1214741329302331\n",
      "0.10168323430324643\n",
      "0.07952008003835218\n",
      "0.06321776885264489\n",
      "0.04547685268410866\n",
      "0.02631376166863068\n",
      "0.005834017287473963\n",
      "0.0001956507966063021\n",
      "5.434260486788731e-06\n",
      "7.1153557403697e-08\n",
      "1.9447786801102263e-09\n",
      "2.6235987340972322e-11\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init.run()\n",
    "    G.read_prob('prob 4-3h a perturbed.txt')\n",
    "    lr = 0.1\n",
    "    counter = 0\n",
    "    while True:\n",
    "        for i in range(200):\n",
    "            training_op.run(feed_dict = {learning_rate: lr})\n",
    "        temp = target_loss.eval()  \n",
    "        print(temp)\n",
    "        lr = min(temp*10, lr)\n",
    "        counter += 1\n",
    "        if counter % 10 == 0:\n",
    "            saver.save(sess, \"Bayes/Save.ckpt\")    \n",
    "        if (temp < 1e-9) or (counter >= 50):\n",
    "            saver.save(sess, \"Bayes/Save.ckpt\")   \n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load up the checkpoint and compare the discovered parameters with the original parameters in 'prob 4-3h a perturbed.txt':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from Bayes/Save.ckpt\n",
      "a 0 0.34130945726920364\n",
      "a 1 0.6586905427307963\n",
      "b (0, 0) 0.5125407283592927\n",
      "b (0, 1) 0.48745927164070735\n",
      "b (1, 0) 0.8933298265933285\n",
      "b (1, 1) 0.10667017340667151\n",
      "c (0, 0) 0.08060593741515065\n",
      "c (0, 1) 0.9193940625848493\n",
      "c (1, 0) 0.6739409491924201\n",
      "c (1, 1) 0.3260590508075799\n",
      "d (0, 0) 0.6411257311081043\n",
      "d (0, 1) 0.35887426889189566\n",
      "d (1, 0) 0.2328541538614901\n",
      "d (1, 1) 0.7671458461385099\n",
      "e (0, 0, 0, 0, 0) 0.0792292225016149\n",
      "e (0, 0, 0, 0, 1) 0.9207707774983851\n",
      "e (0, 0, 0, 1, 0) 0.5150950739187724\n",
      "e (0, 0, 0, 1, 1) 0.4849049260812276\n",
      "e (0, 0, 1, 0, 0) 0.6346259144832795\n",
      "e (0, 0, 1, 0, 1) 0.36537408551672046\n",
      "e (0, 0, 1, 1, 0) 0.44030436942985174\n",
      "e (0, 0, 1, 1, 1) 0.5596956305701483\n",
      "e (0, 1, 0, 0, 0) 0.5912148034174216\n",
      "e (0, 1, 0, 0, 1) 0.4087851965825784\n",
      "e (0, 1, 0, 1, 0) 0.1697487039754937\n",
      "e (0, 1, 0, 1, 1) 0.8302512960245063\n",
      "e (0, 1, 1, 0, 0) 0.2202475397524286\n",
      "e (0, 1, 1, 0, 1) 0.7797524602475714\n",
      "e (0, 1, 1, 1, 0) 0.5648666996919314\n",
      "e (0, 1, 1, 1, 1) 0.4351333003080686\n",
      "e (1, 0, 0, 0, 0) 0.9605092371778331\n",
      "e (1, 0, 0, 0, 1) 0.0394907628221669\n",
      "e (1, 0, 0, 1, 0) 0.29937901725792704\n",
      "e (1, 0, 0, 1, 1) 0.700620982742073\n",
      "e (1, 0, 1, 0, 0) 0.043602590277400176\n",
      "e (1, 0, 1, 0, 1) 0.9563974097225998\n",
      "e (1, 0, 1, 1, 0) 0.2042211783605017\n",
      "e (1, 0, 1, 1, 1) 0.7957788216394983\n",
      "e (1, 1, 0, 0, 0) 0.5838318678286435\n",
      "e (1, 1, 0, 0, 1) 0.4161681321713565\n",
      "e (1, 1, 0, 1, 0) 0.6449785699142304\n",
      "e (1, 1, 0, 1, 1) 0.3550214300857696\n",
      "e (1, 1, 1, 0, 0) 0.8428682858070935\n",
      "e (1, 1, 1, 0, 1) 0.1571317141929065\n",
      "e (1, 1, 1, 1, 0) 0.6103551924757439\n",
      "e (1, 1, 1, 1, 1) 0.3896448075242561\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    saver.restore(sess, \"Bayes/Save.ckpt\")\n",
    "    for i,v in G.vlist.items():\n",
    "        if len(v.parents) == 0:\n",
    "            for j, k in v.dist.items():\n",
    "                print(v.vname, j,k.eval())\n",
    "        else:\n",
    "            for j, k in v.trans.items():\n",
    "                print(v.vname, j,k.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to numerical error, it is the same as the original parameters.\n",
    "\n",
    "In summary, the failure to converge to a global minimum has made detecting non-identifiability with infinite preimage equally hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we examine the discrepency when we interpolate linearly between the two sets of parameters differing by flipping the hidden label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.0408340855860843e-17\n",
      "1 0.017224183567053485\n",
      "2 0.034186686674072105\n",
      "3 0.050882226697932006\n",
      "4 0.06730562993557461\n",
      "5 0.08345183160400434\n",
      "6 0.09931587584029214\n",
      "7 0.11489291570157129\n",
      "8 0.13017821316504058\n",
      "9 0.14516713912796314\n",
      "10 0.1598551734076662\n",
      "11 0.17423790474154116\n",
      "12 0.18831103078704475\n",
      "13 0.20207035812169727\n",
      "14 0.2155118022430836\n",
      "15 0.22863138756885337\n",
      "16 0.2414252474367199\n",
      "17 0.2538896241044615\n",
      "18 0.2660208687499208\n",
      "19 0.27781544147100506\n",
      "20 0.28926991128568524\n",
      "21 0.30038095613199706\n",
      "22 0.31114536286804106\n",
      "23 0.32156002727198163\n",
      "24 0.3316219540420474\n",
      "25 0.34132825679653217\n",
      "26 0.3506761580737935\n",
      "27 0.3596629893322537\n",
      "28 0.36828619095039933\n",
      "29 0.37654331222678117\n",
      "30 0.38443201138001487\n",
      "31 0.39195005554877993\n",
      "32 0.39909532079182086\n",
      "33 0.4058657920879458\n",
      "34 0.4122595633360281\n",
      "35 0.418274837355005\n",
      "36 0.42390992588387827\n",
      "37 0.42916324958171403\n",
      "38 0.4340333380276431\n",
      "39 0.43851882972086026\n",
      "40 0.4426184720806247\n",
      "41 0.44633112144626064\n",
      "42 0.4496557430771563\n",
      "43 0.45259141115276363\n",
      "44 0.4551373087726003\n",
      "45 0.45729272795624737\n",
      "46 0.4590570696433507\n",
      "47 0.4604298436936206\n",
      "48 0.46141066888683147\n",
      "49 0.46199927292282233\n",
      "50 0.4621954924214967\n",
      "51 0.46199927292282233\n",
      "52 0.46141066888683147\n",
      "53 0.4604298436936206\n",
      "54 0.4590570696433507\n",
      "55 0.45729272795624737\n",
      "56 0.4551373087726003\n",
      "57 0.45259141115276375\n",
      "58 0.44965574307715617\n",
      "59 0.44633112144626064\n",
      "60 0.44261847208062466\n",
      "61 0.43851882972086026\n",
      "62 0.4340333380276431\n",
      "63 0.42916324958171403\n",
      "64 0.42390992588387827\n",
      "65 0.41827483735500487\n",
      "66 0.4122595633360281\n",
      "67 0.4058657920879459\n",
      "68 0.39909532079182075\n",
      "69 0.39195005554878004\n",
      "70 0.38443201138001476\n",
      "71 0.3765433122267812\n",
      "72 0.36828619095039933\n",
      "73 0.3596629893322536\n",
      "74 0.3506761580737935\n",
      "75 0.3413282567965322\n",
      "76 0.3316219540420475\n",
      "77 0.3215600272719816\n",
      "78 0.3111453628680412\n",
      "79 0.3003809561319971\n",
      "80 0.28926991128568524\n",
      "81 0.27781544147100506\n",
      "82 0.26602086874992087\n",
      "83 0.2538896241044615\n",
      "84 0.2414252474367199\n",
      "85 0.22863138756885337\n",
      "86 0.21551180224308358\n",
      "87 0.20207035812169727\n",
      "88 0.18831103078704475\n",
      "89 0.17423790474154122\n",
      "90 0.15985517340766614\n",
      "91 0.14516713912796314\n",
      "92 0.13017821316504058\n",
      "93 0.11489291570157129\n",
      "94 0.09931587584029214\n",
      "95 0.08345183160400434\n",
      "96 0.06730562993557461\n",
      "97 0.05088222669793201\n",
      "98 0.0341866866740721\n",
      "99 0.017224183567053568\n",
      "100 1.0408340855860843e-17\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(config=config) as sess:\n",
    "    init2.run()\n",
    "    G.read_prob_interpolate('prob 4-3h a.txt', 'prob 4-3h b.txt', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it is in fact a nice single peaked function. Even though the preimage is infinite in the parameter space, the submanifold of preimage does not seem to intersect the segment connecting these two points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GPU]",
   "language": "python",
   "name": "conda-env-GPU-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
